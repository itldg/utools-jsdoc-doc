const fs = require('fs')
const path = require('path')
const https = require('https')
const utils = require('./utils.js')
const crypto = require('crypto')

const URL_BASE = 'https://www.jsdoc.com.cn'

/**
 * @typedef UrlItem ç½‘å€é¡¹
 * @property {String} title æ ‡é¢˜
 * @property {String} url ç½‘å€
 */

/**
 * è·å–é“¾æ¥åˆ—è¡¨
 * @return {Promise<UrlItem[]>} é“¾æ¥åˆ—è¡¨
 */
async function getLinks() {
	const html = await utils.httpGet(URL_BASE)
	const regex = /<a class="menu__link[^"]*?" [^>]*?tabindex="0" href="(.*?)">(.*?)<\/a>/gm
	const links = []
	let m
	while ((m = regex.exec(html)) !== null) {
		if (m.index === regex.lastIndex) {
			regex.lastIndex++
		}
		links.push({
			url: m[1],
			title: m[2].replace(/[\u0000-\u001F\u007F-\u009F]/g, ''),
		})
	}
	return links
}

/**è·å–æ–‡æ¡£æ‘˜è¦
 * @param {String} docHtml æ–‡æ¡£Htmlå†…å®¹
 * @return {String} æ–‡æ¡£æ‘˜è¦,è·å–å¤±è´¥è¿”å›æš‚æ— æè¿°
 */
function getDocSummary(docHtml) {
	const regex = /<h2 class="anchor[^"]*?" id="(æ¦‚è¿°|ä»‹ç»)">\1<a href="#\1" class="hash-link" aria-label="\1çš„ç›´æ¥é“¾æ¥" title="\1çš„ç›´æ¥é“¾æ¥">.*?<\/a><\/h2>([\s\S]*?)<h2/g
	const matchs = regex.exec(docHtml)
	if (!matchs) {
		return ''
	}
	return utils.removeHtmlTag(matchs[2])
}

/**è·å–æ–‡ç« å†…å®¹
 * @param {UrlItem} url ç½‘å€é¡¹
 * @return {Promise<utils.ArticleInfo>} æ–‡ç« ä¿¡æ¯
 */
async function getArticle(url) {
	const html = await utils.httpGet(URL_BASE + url.url)
	const regex = /<div class="theme-doc-markdown markdown">([\s\S]*?)<\/article>/
	const m = regex.exec(html)
	if (!m) {
		throw new Error('æœªæ‰¾åˆ°æ–‡ç« å†…å®¹')
	}
	let docHtml = m[1]
	const summary = getDocSummary(docHtml)
	docHtml = `<!DOCTYPE html><html lang="zh_CN"><head><meta charset="UTF-8"><title></title><link rel="stylesheet" href="../doc.css" /></head> <body>${docHtml}</body></html>`
	//ç§»é™¤å¤åˆ¶æŒ‰é’®
	docHtml = docHtml.replace(/<div class="buttonGroup__atx">.*?<\/div>/g, '')
	//æ›´æ–°é“¾æ¥
	const links = docHtml.match(/<a[^>\n]+?href="[^"\n]+?"/g)
	if (links) {
		// é“¾æ¥é›†åˆ
		const linkSet = new Set(links)
		for (let link of linkSet) {
			let url = link.match(/<a[^>\n]+?href="([^"\n]+?)"/)[1].trim()
			//å¤–éƒ¨é“¾æ¥æ— éœ€å¤„ç†
			if (/^https?:\/\//i.test(url)) continue
			//æ›¿æ¢æ–‡æ¡£é“¾æ¥
			let anchor = ''
			if (url.includes('#')) {
				anchor = url.substring(url.indexOf('#'))
				url = url.substring(0, url.indexOf('#'))
			}

			const localFile = crypto.createHash('md5').update(url).digest('hex')
			let replaceText = 'href="' + url + '"'
			docHtml = docHtml.replace(new RegExp(replaceText.replace(/[.*+?^${}()|[\]\\]/g, '\\$&'), 'g'), 'href="' + localFile + '.html' + anchor + '"')
		}
	}
    //æ‰€æœ‰ä»£ç å—éƒ½æŒ‰JSé«˜äº®
    docHtml = docHtml.replace(/<pre><code class="lang-[^"]*?">/g, '<pre><code class="lang-js">')
	const filename = crypto.createHash('md5').update(url.url.toLowerCase()).digest('hex')
	fs.writeFileSync(path.join(process.cwd(), 'dist', 'docs', filename + '.html'), docHtml)
	return { t: url.title, p: 'docs/' + filename + '.html', d: summary }
}

async function main() {
	const links = await getLinks()
	console.log('ğŸ“š', 'å…±è®¡ ' + links.length + ' ç¯‡æ–‡æ¡£')

	const docDir = path.join(__dirname, 'dist', 'docs')
	if (!fs.existsSync(docDir)) {
		fs.mkdirSync(docDir)
	}

	/**
	 * @type {utils.ArticleInfo[]}
	 */
	let indexes = []
	const lenStrLen = String(links.length).length
	for (let i = 0; i < links.length; i++) {
		const logStart = `[${String(i + 1).padStart(lenStrLen, '0')}/${links.length}]`
		const item = links[i]
		try {
			let articleInfo = await getArticle(item)
			indexes.push(articleInfo)
		} catch (e) {
			utils.printClearLine()
			console.log(`${logStart} ğŸ’¢ ${e.message}`)
			continue
		}
		utils.printCurrrLine(`${logStart} âœ… ${item.src}`)
	}
	utils.printClearLine()
	const indexesFilePath = path.join(docDir, 'indexes.json')
	fs.writeFileSync(indexesFilePath, JSON.stringify(indexes))
	await utils.updateReadMe(indexes)

	console.log('--------  ğŸ˜ å…¨éƒ¨å®Œæˆ,å…±è®¡' + indexes.length + 'ç¯‡æ–‡æ¡£ --------')
	process.exit(0)
}
main()
